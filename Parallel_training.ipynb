{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPh552+LBjpLh5X+4gBEtK9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talkin24/jaxflax_lab/blob/main/Parallel_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensembling on multiple devices"
      ],
      "metadata": {
        "id": "SaxTBxAqXfY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "앙상블의 크기가 사용 가능한 디바이스 수와 동일한 MNIST 데이터 세트에서 CNN의 앙상블을 훈련하는 방법을 보여드리겠습니다. 간단히 설명하면 다음과 같습니다:\n",
        "\n",
        "- `jax.pmap()`을 사용하여 여러 함수를 병렬로 만듭니다,\n",
        "\n",
        "- 랜덤 시드를 분할하여 다른 매개변수 초기화를 얻습니다,\n",
        "\n",
        "- 입력을 복제하고 필요한 경우 출력을 복제 해제합니다,\n",
        "\n",
        "- 예측을 계산하기 위해 여러 기기에서 평균 확률을 계산합니다.\n",
        "\n",
        "이 하우투에서는 임포트, CNN 모듈, 메트릭 계산과 같은 일부 코드를 생략했지만 이러한 코드는 MNIST 예제에서 찾을 수 있습니다."
      ],
      "metadata": {
        "id": "RaB0xEX0ZSha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel functions\n",
        "\n",
        "먼저 모델의 초기 파라미터를 검색하는 `create_train_state()`의 병렬 버전을 생성합니다. 이 작업은 `jax.pmap()`을 사용하여 수행합니다. 함수를 \"pmap\"하는 효과는 함수를 XLA로 컴파일하지만(jax.jit()와 유사), XLA 디바이스(예: GPU/TPU)에서 병렬로 실행하는 것입니다."
      ],
      "metadata": {
        "id": "tiENrVX8ZiT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single-model\n",
        "def create_train_state(rng, learning_rate, momentum):\n",
        "  cnn = CNN()\n",
        "  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
        "  tx = optax.sgd(learning_rate, momentum)\n",
        "  return train_state.TrainState.create(\n",
        "      apply_fn=cnn.apply, params=params, tx=tx)\n",
        "  \n",
        "\n",
        "# Ensemble\n",
        "@functools.partial(jax.pmap, static_broadcasted_argnums=(1, 2)) #####\n",
        "def create_train_state(rng, learning_rate, momentum):\n",
        "  cnn = CNN()\n",
        "  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
        "  tx = optax.sgd(learning_rate, momentum)\n",
        "  return train_state.TrainState.create(\n",
        "      apply_fn=cnn.apply, params=params, tx=tx)"
      ],
      "metadata": {
        "id": "-zvyq-nQZpZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 단일 모델 코드의 경우 `jax.jit()`를 사용하여 모델을 느리게 초기화합니다. \n",
        "\n",
        "앙상블의 경우 `jax.pmap()`은 기본적으로 제공된 인수 `rng`의 첫 번째 축에 매핑되므로 나중에 이 함수를 호출할 때 각 디바이스에 대해 다른 값을 제공해야 합니다.\n",
        "\n",
        "또한 `learning_rate`와 `momentum`을 정적 인자로 지정하여 추상적인 모양이 아닌 이 인자의 구체적인 값을 사용하도록 지정한 점에 유의하세요. 이는 제공된 인수가 스칼라 값이기 때문에 필요합니다. \n",
        "\n",
        "다음으로 `apply_model()` 및 `update_model()` 함수에 대해서도 동일한 작업을 수행합니다. \n",
        "\n",
        "앙상블에서 예측을 계산하기 위해 개별 확률의 평균을 취합니다. `jax.lax.pmean()`을 사용하여 여러 기기에서 평균을 계산합니다. 이를 위해서는 `jax.pmap()` 및 `jax.lax.pmean()` 모두에 axis_name을 지정해야 합니다."
      ],
      "metadata": {
        "id": "5-cUTwCqaPCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Single-model\n",
        "@jax.jit #####\n",
        "def apply_model(state, images, labels):\n",
        "  def loss_fn(params):\n",
        "    logits = CNN().apply({'params': params}, images)\n",
        "    one_hot = jax.nn.one_hot(labels, 10)\n",
        "    loss = optax.softmax_cross_entropy(logits=logits, labels=one_hot).mean()\n",
        "    return loss, logits\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, logits), grads = grad_fn(state.params)\n",
        "\n",
        "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels) #####\n",
        "  return grads, loss, accuracy\n",
        "\n",
        "@jax.jit #####\n",
        "def update_model(state, grads):\n",
        "  return state.apply_gradients(grads=grads)\n",
        "\n",
        "\n",
        "# Ensemble\n",
        "@functools.partial(jax.pmap, axis_name='ensemble') #####\n",
        "def apply_model(state, images, labels):\n",
        "  def loss_fn(params):\n",
        "    logits = CNN().apply({'params': params}, images)\n",
        "    one_hot = jax.nn.one_hot(labels, 10)\n",
        "    loss = optax.softmax_cross_entropy(logits=logits, labels=one_hot).mean()\n",
        "    return loss, logits\n",
        "\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, logits), grads = grad_fn(state.params)\n",
        "  probs = jax.lax.pmean(jax.nn.softmax(logits), axis_name='ensemble') #####\n",
        "  accuracy = jnp.mean(jnp.argmax(probs, -1) == labels) #####\n",
        "  return grads, loss, accuracy\n",
        "\n",
        "@jax.pmap #####\n",
        "def update_model(state, grads):\n",
        "  return state.apply_gradients(grads=grads)"
      ],
      "metadata": {
        "id": "zRXl5353a1UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Ensemble\n",
        "\n",
        "다음으로 `train_epoch()` 함수를 변환합니다.\n",
        "\n",
        "위에서 pmapped 함수를 호출할 때 필요한 경우 모든 디바이스에 대한 인수를 복제하고 반환값의 중복을 제거하는 작업을 주로 처리해야 합니다."
      ],
      "metadata": {
        "id": "NGJNAGcOOFYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single-model\n",
        "def train_epoch(state, train_ds, batch_size, rng):\n",
        "  train_ds_size = len(train_ds['image'])\n",
        "  steps_per_epoch = train_ds_size // batch_size\n",
        "\n",
        "  perms = jax.random.permutation(rng, len(train_ds['image']))\n",
        "  perms = perms[:steps_per_epoch * batch_size]\n",
        "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
        "\n",
        "  epoch_loss = []\n",
        "  epoch_accuracy = []\n",
        "\n",
        "  for perm in perms:\n",
        "    batch_images = train_ds['image'][perm, ...] #####\n",
        "    batch_labels = train_ds['label'][perm, ...] #####\n",
        "    grads, loss, accuracy = apply_model(state, batch_images, batch_labels)\n",
        "    state = update_model(state, grads)\n",
        "    epoch_loss.append(loss) #####\n",
        "    epoch_accuracy.append(accuracy) #####\n",
        "  train_loss = np.mean(epoch_loss)\n",
        "  train_accuracy = np.mean(epoch_accuracy)\n",
        "  return state, train_loss, train_accuracy\n",
        "\n",
        "\n",
        "# Ensemble\n",
        "def train_epoch(state, train_ds, batch_size, rng):\n",
        "  train_ds_size = len(train_ds['image'])\n",
        "  steps_per_epoch = train_ds_size // batch_size\n",
        "\n",
        "  perms = jax.random.permutation(rng, len(train_ds['image']))\n",
        "  perms = perms[:steps_per_epoch * batch_size]\n",
        "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
        "\n",
        "  epoch_loss = []\n",
        "  epoch_accuracy = []\n",
        "\n",
        "  for perm in perms:\n",
        "    batch_images = jax_utils.replicate(train_ds['image'][perm, ...]) #####\n",
        "    batch_labels = jax_utils.replicate(train_ds['label'][perm, ...]) #####\n",
        "    grads, loss, accuracy = apply_model(state, batch_images, batch_labels)\n",
        "    state = update_model(state, grads)\n",
        "    epoch_loss.append(jax_utils.unreplicate(loss)) #####\n",
        "    epoch_accuracy.append(jax_utils.unreplicate(accuracy)) #####\n",
        "  train_loss = np.mean(epoch_loss)\n",
        "  train_accuracy = np.mean(epoch_accuracy)\n",
        "  return state, train_loss, train_accuracy"
      ],
      "metadata": {
        "id": "Ty8H1NVUOXL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "보시다시피 `state`와 관련된 로직을 변경할 필요가 없습니다. \n",
        "\n",
        "아래 훈련 코드에서 볼 수 있듯이 train state는 이미 복제되어 있으므로 `train_step()`에 전달하면 `train_step()`이 pmap되어 있기 때문에 정상적으로 작동하기 때문입니다. 그러나 훈련 데이터 세트는 아직 복제되지 않았으므로 여기서 리플리케이션을 수행합니다. 전체 train 데이터 집합을 복제하는 것은 너무 많은 메모리를 사용하므로 배치 수준에서 수행합니다.\n",
        "\n",
        "이제 실제 훈련 로직을 다시 작성할 수 있습니다. 이는 두 가지 간단한 변경 사항으로 구성됩니다. RNG를 `create_train_state()`에 전달할 때 복제되는지 확인하는 것과 전체 데이터 세트에 대해 직접 이 작업을 수행할 수 있도록 훈련 데이터 세트보다 훨씬 작은 테스트 데이터 세트를 리플리케이트하는 것입니다."
      ],
      "metadata": {
        "id": "w9fgkrSlPTra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single-model\n",
        "train_ds, test_ds = get_datasets()\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "rng, init_rng = jax.random.split(rng)\n",
        "state = create_train_state(init_rng, learning_rate, momentum) #####\n",
        "\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "  rng, input_rng = jax.random.split(rng)\n",
        "  state, train_loss, train_accuracy = train_epoch(\n",
        "      state, train_ds, batch_size, input_rng)\n",
        "\n",
        "  _, test_loss, test_accuracy = apply_model( #####\n",
        "      state, test_ds['image'], test_ds['label']) #####\n",
        "\n",
        "  logging.info(\n",
        "      'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, '\n",
        "      'test_loss: %.4f, test_accuracy: %.2f'\n",
        "      % (epoch, train_loss, train_accuracy * 100, test_loss,\n",
        "         test_accuracy * 100))\n",
        "\n",
        "\n",
        "# Ensemble\n",
        "train_ds, test_ds = get_datasets()\n",
        "test_ds = jax_utils.replicate(test_ds) #####\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "rng, init_rng = jax.random.split(rng)\n",
        "state = create_train_state(jax.random.split(init_rng, jax.device_count()), #####\n",
        "                           learning_rate, momentum) #####\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "  rng, input_rng = jax.random.split(rng)\n",
        "  state, train_loss, train_accuracy = train_epoch(\n",
        "      state, train_ds, batch_size, input_rng)\n",
        "\n",
        "  _, test_loss, test_accuracy = jax_utils.unreplicate( #####\n",
        "      apply_model(state, test_ds['image'], test_ds['label'])) #####\n",
        "\n",
        "  logging.info(\n",
        "      'epoch:% 3d, train_loss: %.4f, train_accuracy: %.2f, '\n",
        "      'test_loss: %.4f, test_accuracy: %.2f'\n",
        "      % (epoch, train_loss, train_accuracy * 100, test_loss,\n",
        "         test_accuracy * 100))"
      ],
      "metadata": {
        "id": "RdETw_CXPvtB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}