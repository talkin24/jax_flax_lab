{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrdrwadE1vuMuSLNmcjg0o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talkin24/jaxflax_lab/blob/main/Training_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch normalization\n",
        "\n",
        "ë°°ì¹˜ ì •ê·œí™”ëŠ” í›ˆë ¨ ì†ë„ë¥¼ ë†’ì´ê³  ìˆ˜ë ´ì„ ê°œì„ í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ì •ê·œí™” ê¸°ë²•. í›ˆë ¨ ì¤‘ì— íŠ¹ì§• ì°¨ì›ì— ëŒ€í•œ ëŸ¬ë‹ì—ë²„ë¦¬ì§€ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì ì ˆí•˜ê²Œ ì²˜ë¦¬í•´ì•¼ í•˜ëŠ” ìƒˆë¡œìš´ í˜•íƒœì˜ ë¯¸ë¶„ë¶ˆê°€ëŠ¥ ìƒíƒœê°€ ì¶”ê°€ë©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "rT8HPqT5Sb-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the model with `BatchNorm`"
      ],
      "metadata": {
        "id": "UfZL_40mTHT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flaxì—ì„œ `BatchNorm`ì€ í›ˆë ¨ê³¼ ì¶”ë¡  ê°„ì— ì„œë¡œ ë‹¤ë¥¸ ëŸ°íƒ€ì„ ë™ì‘ì„ ë³´ì´ëŠ” `flax.linen.Module`. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ use_running_average ì¸ìˆ˜ë¥¼ í†µí•´ ëª…ì‹œì ìœ¼ë¡œ ì§€ì • ê°€ëŠ¥\n",
        "\n",
        "ì¼ë°˜ì ì¸ íŒ¨í„´ì€ ë¶€ëª¨ Flax ëª¨ë“ˆì—ì„œ train(í›ˆë ¨) ì¸ìˆ˜ë¥¼ ìˆ˜ë½í•˜ê³ , ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ BatchNormì˜ use_running_average ì¸ìˆ˜ë¥¼ ì •ì˜í•  ìˆ˜ ìˆìŒ\n",
        "\n",
        "ì°¸ê³ : PyTorchë‚˜ TensorFlow(Keras)ì™€ ê°™ì€ ë‹¤ë¥¸ ë¨¸ì‹  ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì—ì„œëŠ” ë³€ê²½ ê°€ëŠ¥í•œ ìƒíƒœ ë˜ëŠ” í˜¸ì¶œ í”Œë˜ê·¸ë¥¼ í†µí•´ ì§€ì •í•©ë‹ˆë‹¤(ì˜ˆ: torch.nn.Module.eval ë˜ëŠ” tf.keras.Modelì—ì„œ í›ˆë ¨ í”Œë˜ê·¸ë¥¼ ì„¤ì •).\n"
      ],
      "metadata": {
        "id": "ODI4LOZfTMsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No BatchNorm\n",
        "class MLP(nn.Module):\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = nn.Dense(features=4)(x)\n",
        "\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Dense(features=1)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "6wBLW0u-cloP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With BatchNorm\n",
        "class MLP(nn.Module):\n",
        "  @nn.compact\n",
        "  def __call__(self, x, train: bool):\n",
        "    x = nn.Dense(features=4)(x)\n",
        "    x = nn.BatchNorm(use_running_average=not train)(x)\n",
        "    x = nn.relu(x)\n",
        "    x = nn.Dense(features=1)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "exHJJAkfcpH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ëª¨ë¸ì„ ìƒì„±í•œ í›„ì—ëŠ” `flax.linen.init()`ì„ í˜¸ì¶œí•˜ì—¬ ë³€ìˆ˜ êµ¬ì¡°ë¥¼ ê°€ì ¸ì™€ ì´ˆê¸°í™”í•¨. ì—¬ê¸°ì„œ BatchNormì´ ì—†ëŠ” ì½”ë“œì™€ BatchNormì´ ìˆëŠ” ì½”ë“œì˜ ì£¼ìš” ì°¨ì´ì ì€ train ì¸ìˆ˜ë¥¼ ì œê³µí•´ì•¼ í•œë‹¤ëŠ” ê²ƒ."
      ],
      "metadata": {
        "id": "CgglU8FBct8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The `batch_stats` collection\n",
        "\n",
        "`params` ì»¬ë ‰ì…˜ ì™¸ì—ë„, BatchNormì€ ë°°ì¹˜ í†µê³„ì˜ ëŸ¬ë‹ ì—ë²„ë¦¬ì§€ë¥¼ í¬í•¨í•˜ëŠ” `batch_stats` ì»¬ë ‰ì…˜ë„ ì¶”ê°€\n",
        "\n",
        "ì°¸ê³ : ìì„¸í•œ ë‚´ìš©ì€ `flax.linen` ë³€ìˆ˜ API ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŒ\n",
        "\n",
        "ë‚˜ì¤‘ì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë³€ìˆ˜ì—ì„œ `batch_stats` ì»¬ë ‰ì…˜ì„ ì¶”ì¶œí•´ì•¼ í•¨"
      ],
      "metadata": {
        "id": "4PcBn32NdOy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No BatchNorm\n",
        "mlp = MLP()\n",
        "x = jnp.ones((1, 3))\n",
        "variables = mlp.init(jax.random.PRNGKey(0), x)\n",
        "params = variables['params']\n",
        "\n",
        "\n",
        "jax.tree_util.tree_map(jnp.shape, variables)"
      ],
      "metadata": {
        "id": "epT9aBpRdijy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wtih BatchNorm\n",
        "mlp = MLP()\n",
        "x = jnp.ones((1, 3))\n",
        "variables = mlp.init(jax.random.PRNGKey(0), x, train=False)\n",
        "params = variables['params']\n",
        "batch_stats = variables['batch_stats']\n",
        "\n",
        "jax.tree_util.tree_map(jnp.shape, variables)"
      ],
      "metadata": {
        "id": "uvu4eXlodo_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flax `BatchNorm`ì€ ì´ 4ê°œì˜ ë³€ìˆ˜ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. `batch_stats` ì»¬ë ‰ì…˜ì— ìˆëŠ” `mean`ê³¼ `var`, ê·¸ë¦¬ê³  `params` ì»¬ë ‰ì…˜ì— ìˆëŠ” `scale`ì™€ `bias`ì…ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "vThE5lPAd7DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No BatchNorm\n",
        "FrozenDict({\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  'params': {\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    'Dense_0': {\n",
        "        'bias': (4,),\n",
        "        'kernel': (3, 4),\n",
        "    },\n",
        "    'Dense_1': {\n",
        "        'bias': (1,),\n",
        "        'kernel': (4, 1),\n",
        "    },\n",
        "  },\n",
        "})"
      ],
      "metadata": {
        "id": "6gObEyM2eFVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with BatchNorm\n",
        "FrozenDict({\n",
        "  'batch_stats': {\n",
        "    'BatchNorm_0': {\n",
        "        'mean': (4,),\n",
        "        'var': (4,),\n",
        "    },\n",
        "  },\n",
        "  'params': {\n",
        "    'BatchNorm_0': {\n",
        "        'bias': (4,),\n",
        "        'scale': (4,),\n",
        "    },\n",
        "    'Dense_0': {\n",
        "        'bias': (4,),\n",
        "        'kernel': (3, 4),\n",
        "    },\n",
        "    'Dense_1': {\n",
        "        'bias': (1,),\n",
        "        'kernel': (4, 1),\n",
        "    },\n",
        "  },\n",
        "})"
      ],
      "metadata": {
        "id": "-UO463UIeKgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modifying `flax.linen.apply`\n",
        "\n",
        "`flax.linen.apply`ë¥¼ ì‚¬ìš©í•˜ì—¬ `train==True` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì‹¤í–‰í•  ë•Œ(ì¦‰, `BatchNorm` í˜¸ì¶œì— `use_running_average==False`ê°€ ìˆëŠ” ê²½ìš°) ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤:\n",
        "\n",
        "- `batch_stats`ë¥¼ ì…ë ¥ ë³€ìˆ˜ë¡œ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "- `batch_stats` ì»¬ë ‰ì…˜ì€ `mutable=['batch_stats']`ë¥¼ ì„¤ì •í•˜ì—¬ ë³€ê²½ ê°€ëŠ¥í•œ ê²ƒìœ¼ë¡œ í‘œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "- ë³€ê²½ëœ ë³€ìˆ˜ëŠ” ë‘ ë²ˆì§¸ ì¶œë ¥ìœ¼ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤ . ì—…ë°ì´íŠ¸ëœ `batch_stats`ëŠ” ì—¬ê¸°ì—ì„œ ì¶”ì¶œí•´ì•¼ í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "w_ZlI5maeVcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No BatchNorm\n",
        "y = mlp.apply(\n",
        "  {'params': params},\n",
        "  x,\n",
        "\n",
        ")\n",
        "..."
      ],
      "metadata": {
        "id": "FRni_nX5erkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With BatchNorm\n",
        "y, updates = mlp.apply(\n",
        "  {'params': params, 'batch_stats': batch_stats},\n",
        "  x,\n",
        "  train=True, mutable=['batch_stats']\n",
        ")\n",
        "batch_stats = updates['batch_stats']"
      ],
      "metadata": {
        "id": "ZFxKENt4es8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and evaluation\n",
        "\n",
        "`BatchNorm`ì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì„ í›ˆë ¨ ë£¨í”„ì— í†µí•©í•  ë•Œ ê°€ì¥ ì–´ë ¤ìš´ ì ì€ ì¶”ê°€ì ì¸ `batch_stats` ìƒíƒœë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë ¤ë©´ ë‹¤ìŒì„ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤:\n",
        "\n",
        "- ì‚¬ìš©ì ì •ì˜ `flax.training.train_state.TrainState` í´ë˜ìŠ¤ì— batch_stats í•„ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "\n",
        "- `batch_stats` ê°’ì„ `train_state.TrainState.create` ë©”ì„œë“œì— ì „ë‹¬í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "JknpZOIte_4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No BatchNorm\n",
        "from flax.training import train_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "state = train_state.TrainState.create(\n",
        "  apply_fn=mlp.apply,\n",
        "  params=params,\n",
        "\n",
        "  tx=optax.adam(1e-3),\n",
        ")"
      ],
      "metadata": {
        "id": "v0tvvdyVfQu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With BatchNorm\n",
        "from flax.training import train_state\n",
        "\n",
        "class TrainState(train_state.TrainState):\n",
        "  batch_stats: Any\n",
        "\n",
        "state = TrainState.create(\n",
        "  apply_fn=mlp.apply,\n",
        "  params=params,\n",
        "  batch_stats=batch_stats,\n",
        "  tx=optax.adam(1e-3),\n",
        ")"
      ],
      "metadata": {
        "id": "YBdZU2D3fTjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë˜í•œ ì´ëŸ¬í•œ ë³€ê²½ ì‚¬í•­ì„ ë°˜ì˜í•˜ë„ë¡ `train_step` í•¨ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ì„¸ìš”:\n",
        "\n",
        "- ì´ì „ì— ì„¤ëª…í•œ ëŒ€ë¡œ ëª¨ë“  ìƒˆ ë§¤ê°œë³€ìˆ˜ë¥¼ `flax.linen.apply`ì— ì „ë‹¬í•©ë‹ˆë‹¤.\n",
        "\n",
        "- `batch_stats`ì— ëŒ€í•œ `updates`ëŠ” `loss_fn`ì—ì„œ ì „íŒŒë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "- `TrainState`ì˜ `batch_stats`ë¥¼ ì—…ë°ì´íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "ilqZ2s3UfiaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No BatchNorm\n",
        "@jax.jit\n",
        "def train_step(state: TrainState, batch):\n",
        "  \"\"\"Train for a single step.\"\"\"\n",
        "  def loss_fn(params):\n",
        "    logits = state.apply_fn(\n",
        "      {'params': params},\n",
        "      x=batch['image'])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "      logits=logits, labels=batch['label'])\n",
        "    return loss, logits\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, logits), grads = grad_fn(state.params)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "\n",
        "  metrics = {\n",
        "    'loss': loss,\n",
        "      'accuracy': jnp.mean(jnp.argmax(logits, -1) == batch['label']),\n",
        "  }\n",
        "  return state, metrics"
      ],
      "metadata": {
        "id": "td9uZ9TFfygT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With BatchNorm\n",
        "@jax.jit\n",
        "def train_step(state: TrainState, batch):\n",
        "  \"\"\"Train for a single step.\"\"\"\n",
        "  def loss_fn(params):\n",
        "    logits, updates = state.apply_fn(\n",
        "      {'params': params, 'batch_stats': state.batch_stats},\n",
        "      x=batch['image'], train=True, mutable=['batch_stats'])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "      logits=logits, labels=batch['label'])\n",
        "    return loss, (logits, updates)\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, (logits, updates)), grads = grad_fn(state.params)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  state = state.replace(batch_stats=updates['batch_stats'])\n",
        "  metrics = {\n",
        "    'loss': loss,\n",
        "      'accuracy': jnp.mean(jnp.argmax(logits, -1) == batch['label']),\n",
        "  }\n",
        "  return state, metrics"
      ],
      "metadata": {
        "id": "oT4nqAT_f2E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`eval_step`ì€ í›¨ì”¬ ê°„ë‹¨í•©ë‹ˆë‹¤. `batch_stats`ëŠ” ë³€ê²½ ê°€ëŠ¥í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì—…ë°ì´íŠ¸ë¥¼ ì „íŒŒí•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. `batch_stats`ë¥¼ `flax.linen.apply`ì— ì „ë‹¬í•˜ê³  `train` ì¸ìˆ˜ê°€ `False`ë¡œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”"
      ],
      "metadata": {
        "id": "Wk6Foix2gZch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def eval_step(state: TrainState, batch):\n",
        "  \"\"\"Train for a single step.\"\"\"\n",
        "  logits = state.apply_fn(\n",
        "    {'params': params},\n",
        "    x=batch['image'])\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "    logits=logits, labels=batch['label'])\n",
        "  metrics = {\n",
        "    'loss': loss,\n",
        "      'accuracy': jnp.mean(jnp.argmax(logits, -1) == batch['label']),\n",
        "  }\n",
        "  return state, metrics"
      ],
      "metadata": {
        "id": "I0YutKkCgWB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def eval_step(state: TrainState, batch):\n",
        "  \"\"\"Train for a single step.\"\"\"\n",
        "  logits = state.apply_fn(\n",
        "    {'params': params, 'batch_stats': state.batch_stats},\n",
        "    x=batch['image'], train=False)\n",
        "  loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "    logits=logits, labels=batch['label'])\n",
        "  metrics = {\n",
        "    'loss': loss,\n",
        "      'accuracy': jnp.mean(jnp.argmax(logits, -1) == batch['label']),\n",
        "  }\n",
        "  return state, metrics"
      ],
      "metadata": {
        "id": "ee-YfZKSgkHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropout\n",
        "\n",
        "ì´ ê°€ì´ë“œì—ì„œëŠ” `flax.linen.Dropout()`ì„ ì‚¬ìš©í•˜ì—¬ ë“œë¡­ì•„ì›ƒì„ ì ìš©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ê°œìš”ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
        "\n",
        "ë“œë¡­ì•„ì›ƒì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ ìˆ¨ê²¨ì§„ ë‹¨ìœ„ì™€ ë³´ì´ëŠ” ë‹¨ìœ„ë¥¼ ë¬´ì‘ìœ„ë¡œ ì œê±°í•˜ëŠ” í™•ë¥ ì  ì •ê·œí™” ê¸°ë²•ì…ë‹ˆë‹¤.\n",
        "\n",
        "ê°€ì´ë“œ ì „ì²´ì—ì„œ Flax `Dropout`ì„ ì ìš©í•œ ì½”ë“œ ì˜ˆì œì™€ ì ìš©í•˜ì§€ ì•Šì€ ì½”ë“œ ì˜ˆì œë¥¼ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "Qk58R694SiPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the PRNG key\n",
        "\n",
        "ë“œë¡­ì•„ì›ƒì€ ë¬´ì‘ìœ„ ì—°ì‚°ì´ë¯€ë¡œ ì˜ì‚¬ ë‚œìˆ˜ ìƒì„±ê¸°(PRNG) ìƒíƒœê°€ í•„ìš”í•©ë‹ˆë‹¤. FlaxëŠ” ì¤‘ë¦½ ë„¤íŠ¸ì›Œí¬ì— ë°”ëŒì§í•œ ì—¬ëŸ¬ ê°€ì§€ ì†ì„±ì„ ê°€ì§„ JAXì˜ (ë¶„í•  ê°€ëŠ¥í•œ) PRNG í‚¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ JAX íŠœí† ë¦¬ì–¼ì˜ ì˜ì‚¬ ë‚œìˆ˜ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n",
        "\n",
        "ì°¸ê³ : JAXì—ëŠ” PRNG í‚¤ë¥¼ ì œê³µí•˜ëŠ” ëª…ì‹œì ì¸ ë°©ë²•ì´ ìˆë‹¤ëŠ” ê²ƒì„ ê¸°ì–µí•˜ì„¸ìš”. `key, subkey = jax.random.split(key)`ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ PRNG ìƒíƒœ(ì˜ˆ: `key = jax.random.PRNGKey(seed=0)`)ë¥¼ ì—¬ëŸ¬ ê°œì˜ ìƒˆ PRNG í‚¤ë¡œ í¬í¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸°ì–µì„ ìƒˆë¡œ ê³ ì¹˜ë ¤ë©´ ğŸ”ª JAX - ë‚ ì¹´ë¡œìš´ ë¹„íŠ¸ ğŸ”ª ë¬´ì‘ìœ„ì„± ë° PRNG í‚¤ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "jax.random.split()ì„ ì‚¬ìš©í•˜ì—¬ PRNG í‚¤ë¥¼ Linen `Dropout`ìš© í‚¤ë¥¼ í¬í•¨í•˜ì—¬ 3ê°œì˜ í‚¤ë¡œ ë¶„í• í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "j1yowrndgqpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No Dropout\n",
        "root_key = jax.random.PRNGKey(seed=0)\n",
        "main_key, params_key = jax.random.split(key=root_key)"
      ],
      "metadata": {
        "id": "mzO6Gv1-hQ1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With Dropout\n",
        "root_key = jax.random.PRNGKey(seed=0)\n",
        "main_key, params_key, dropout_key = jax.random.split(key=root_key, num=3)"
      ],
      "metadata": {
        "id": "ypEZSPMmhTLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì°¸ê³ : Flaxì—ì„œëŠ” ë‚˜ì¤‘ì— `flax.linen.Module()`ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ PRNG ìŠ¤íŠ¸ë¦¼ì— ì´ë¦„ì„ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë§¤ê°œë³€ìˆ˜ë¥¼ ì´ˆê¸°í™”í•˜ê¸° ìœ„í•´ `'params'` ìŠ¤íŠ¸ë¦¼ì„ ì „ë‹¬í•˜ê³  `flax.linen.Dropout()`ì„ ì ìš©í•˜ê¸° ìœ„í•´ `'dropout'`ì„ ì „ë‹¬í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "Rz4gZMEFha6T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define your model with `Dropout`\n",
        "\n",
        "\n",
        "ë“œë¡­ì•„ì›ƒì´ ìˆëŠ” ëª¨ë¸ì„ ë§Œë“¤ë ¤ë©´:\n",
        "\n",
        "- `flax.linen.Module()`ì„ ì„œë¸Œí´ë˜ì‹±í•œ ë‹¤ìŒ `flax.linen.Dropout()`ì„ ì‚¬ìš©í•˜ì—¬ ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. `flax.linen.Module()`ì€ ëª¨ë“  ì‹ ê²½ë§ ëª¨ë“ˆì˜ ë² ì´ìŠ¤ í´ë˜ìŠ¤ì´ë©°, ëª¨ë“  ë ˆì´ì–´ì™€ ëª¨ë¸ì€ ì´ í´ë˜ìŠ¤ì—ì„œ ì„œë¸Œí´ë˜ìŠ¤í™”ëœë‹¤ëŠ” ì ì„ ê¸°ì–µí•˜ì„¸ìš”.\n",
        "\n",
        "- `flax.linen.Dropout()`ì—ì„œ `deterministic` ì¸ìˆ˜ëŠ” í‚¤ì›Œë“œ ì¸ìë¡œë„ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤:\n",
        "\n",
        "  - `flax.linen.Module()`ì„ êµ¬ì„±í•  ë•Œ; ë˜ëŠ”\n",
        "  - ìƒì„±ëœ ëª¨ë“ˆì—ì„œ `flax.linen.init()` ë˜ëŠ” `flax.linen.apply()`ë¥¼ í˜¸ì¶œí•  ë•Œ. (ìì„¸í•œ ë‚´ìš©ì€ `flax.linen.module.merge_param()` ì°¸ì¡°).\n",
        "\n",
        "-  `deterministic`ì€ booleanì´ë¯€ë¡œ:\n",
        "\n",
        "  - `False`ë¡œ ì„¤ì •í•˜ë©´ ì…ë ¥ì´ ë§ˆìŠ¤í‚¹ë˜ê³ (ì¦‰, 0ìœ¼ë¡œ ì„¤ì •ë¨) `rate`ì— ë”°ë¼ í™•ë¥ ì´ ì„¤ì •ë©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ë‚˜ë¨¸ì§€ ì…ë ¥ì€ `1 / (1 - ë¹„ìœ¨)`ë¡œ ìŠ¤ì¼€ì¼ë§ë˜ì–´ ì…ë ¥ì˜ í‰ê· ì´ ìœ ì§€ë©ë‹ˆë‹¤.\n",
        "\n",
        "  - `True`ë¡œ ì„¤ì •í•˜ë©´ ë§ˆìŠ¤í¬ê°€ ì ìš©ë˜ì§€ ì•Šê³ (ë“œë¡­ì•„ì›ƒì´ êº¼ì§) ì…ë ¥ì´ ê·¸ëŒ€ë¡œ ë°˜í™˜ë©ë‹ˆë‹¤.\n",
        "\n",
        "ì¼ë°˜ì ì¸ íŒ¨í„´ì€ ë¶€ëª¨ Flax `Module`ì—ì„œ `Training`(ë˜ëŠ” `train`) ì¸ìˆ˜(boolean)ë¥¼ ë°›ì•„ ì´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë“œë¡­ì•„ì›ƒì„ í™œì„±í™” ë˜ëŠ” ë¹„í™œì„±í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤(ì´ ê°€ì´ë“œì˜ ë’·ë¶€ë¶„ì—ì„œ ì„¤ëª…). PyTorchë‚˜ TensorFlow(Keras)ì™€ ê°™ì€ ë‹¤ë¥¸ ë¨¸ì‹  ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì—ì„œëŠ” ë³€ê²½ ê°€ëŠ¥í•œ ìƒíƒœ ë˜ëŠ” í˜¸ì¶œ í”Œë˜ê·¸ë¥¼ í†µí•´ ì§€ì •í•©ë‹ˆë‹¤(ì˜ˆ: `torch.nn.Module.eval` ë˜ëŠ” `tf.keras.Model`ì—ì„œ í›ˆë ¨ í”Œë˜ê·¸ë¥¼ ì„¤ì •í•˜ì—¬).\n",
        "\n",
        "ì°¸ê³ : FlaxëŠ” `Flax flax.linen.Module()`ì˜ `flax.linen.Module.make_rng()` ë©”ì„œë“œë¥¼ í†µí•´ PRNG í‚¤ ìŠ¤íŠ¸ë¦¼ì„ ì•”ì‹œì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ Flax ëª¨ë“ˆ(ë˜ëŠ” ê·¸ í•˜ìœ„ ëª¨ë“ˆ) ë‚´ë¶€ì˜ ìƒˆë¡œìš´ PRNG í‚¤ë¥¼ PRNG ìŠ¤íŠ¸ë¦¼ì—ì„œ ë¶„ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `make_rng` ë©”ì„œë“œëŠ” í˜¸ì¶œí•  ë•Œë§ˆë‹¤ ê³ ìœ í•œ í‚¤ë¥¼ ì œê³µí•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œ `flax.linen.Dropout()`ì€ `flax.linen.Module.make_rng()`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë“œë¡­ì•„ì›ƒì„ ìœ„í•œ í‚¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì†ŒìŠ¤ ì½”ë“œë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš”ì»¨ëŒ€, `flax.linen.Module.make_rng()`ëŠ” ì™„ì „í•œ ì¬í˜„ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "vEOX9MyOhjLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No Dropout\n",
        "class MyModel(nn.Module):\n",
        "  num_neurons: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = nn.Dense(self.num_neurons)(x)\n",
        "\n",
        "\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "BExPuZy7i34v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With Dropout\n",
        "class MyModel(nn.Module):\n",
        "  num_neurons: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, training: bool):\n",
        "    x = nn.Dense(self.num_neurons)(x)\n",
        "    # Set the dropout layer with a `rate` of 50%.\n",
        "    # When the `deterministic` flag is `True`, dropout is turned off.\n",
        "    x = nn.Dropout(rate=0.5, deterministic=not training)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "C3LM3Zi-i8ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the model\n",
        "\n",
        "ëª¨ë¸ì„ ìƒì„±í•œ í›„\n",
        "\n",
        "- ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤.\n",
        "- ê·¸ëŸ° ë‹¤ìŒ `flax.linen.init()` í˜¸ì¶œì—ì„œ `training=False`ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "- ë§ˆì§€ë§‰ìœ¼ë¡œ ë³€ìˆ˜ ì‚¬ì „ì—ì„œ `params`ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
        "\n",
        "ì—¬ê¸°ì„œ Flax `Dropout`ì´ ì—†ëŠ” ì½”ë“œì™€ `Dropout`ì´ ìˆëŠ” ì½”ë“œì˜ ì£¼ìš” ì°¨ì´ì ì€ ë“œë¡­ì•„ì›ƒì„ í™œì„±í™”í•´ì•¼ í•˜ëŠ” ê²½ìš° `training`(ë˜ëŠ” `train`) ì¸ìˆ˜ë¥¼ ì œê³µí•´ì•¼ í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "sjSCI2NFjJan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No Dropout\n",
        "my_model = MyModel(num_neurons=3)\n",
        "x = jnp.empty((3, 4, 4))\n",
        "\n",
        "variables = my_model.init(params_key, x)\n",
        "params = variables['params']"
      ],
      "metadata": {
        "id": "IRXBWb0tjitD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With Dropout\n",
        "my_model = MyModel(num_neurons=3)\n",
        "x = jnp.empty((3, 4, 4))\n",
        "# Dropout is disabled with `training=False` (that is, `deterministic=True`).\n",
        "variables = my_model.init(params_key, x, training=False)\n",
        "params = variables['params']"
      ],
      "metadata": {
        "id": "PbR9-8tLjj_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform the forward pass during training\n",
        "\n",
        "`flax.linen.apply()`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì‹¤í–‰í•  ë•Œ:\n",
        "\n",
        "- `flax.linen.apply()`ì— `training=True`ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤.\n",
        "- ê·¸ëŸ° ë‹¤ìŒ í¬ì›Œë“œ íŒ¨ìŠ¤(ë“œë¡­ì•„ì›ƒ í¬í•¨) ì¤‘ì— PRNG í‚¤ë¥¼ ê·¸ë¦¬ë ¤ë©´ `flax.linen.apply()`ë¥¼ í˜¸ì¶œí•  ë•Œ `'dropout'` ìŠ¤íŠ¸ë¦¼ì„ ì‹œë“œí•  PRNG í‚¤ë¥¼ ì œê³µí•˜ì„¸ìš”."
      ],
      "metadata": {
        "id": "A1PmsMKMjqfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No Dropout\n",
        "# No need to pass the `training` and `rngs` flags.\n",
        "y = my_model.apply({'params': params}, x)"
      ],
      "metadata": {
        "id": "LaCNEmpHj8bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With Dropout\n",
        "# Dropout is enabled with `training=True` (that is, `deterministic=False`).\n",
        "y = my_model.apply({'params': params}, x, training=True, rngs={'dropout': dropout_key})"
      ],
      "metadata": {
        "id": "icF00d6nj-w3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì—¬ê¸°ì„œ flax `Dropout`ì´ ì—†ëŠ” ì½”ë“œì™€ `Dropout`ì´ ìˆëŠ” ì½”ë“œì˜ ì£¼ìš” ì°¨ì´ì ì€ ë“œë¡­ì•„ì›ƒì„ í™œì„±í™”í•´ì•¼ í•˜ëŠ” ê²½ìš° `training`(ë˜ëŠ” `train`) ë° `rngs` ì¸ìˆ˜ë¥¼ ì œê³µí•´ì•¼ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.\n",
        "\n",
        "í‰ê°€ ì¤‘ì—ëŠ” ë“œë¡­ì•„ì›ƒì„ í™œì„±í™”í•˜ì§€ ì•Šì€ ìƒíƒœë¡œ ìœ„ì˜ ì½”ë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤(ì¦‰, RNGë„ ì „ë‹¬í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤)."
      ],
      "metadata": {
        "id": "SR6bSTkkkN2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `TrainState` and the training step\n",
        "\n",
        "ì´ ì„¹ì…˜ì—ì„œëŠ” ë“œë¡­ì•„ì›ƒì„ í™œì„±í™”í•œ ê²½ìš° í•™ìŠµ ë‹¨ê³„ í•¨ìˆ˜ ë‚´ì—ì„œ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
        "\n",
        "ì°¸ê³ : Flaxì—ëŠ” íŒŒë¼ë¯¸í„°ì™€ ì˜µí‹°ë§ˆì´ì € ìƒíƒœë¥¼ í¬í•¨í•˜ì—¬ ì „ì²´ í•™ìŠµ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„° í´ë˜ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ì¼ë°˜ì ì¸ íŒ¨í„´ì´ ìˆë‹¤ëŠ” ì ì„ ê¸°ì–µí•˜ì„¸ìš”. ê·¸ëŸ° ë‹¤ìŒ ë‹¨ì¼ íŒŒë¼ë¯¸í„°ì¸ `state: TrainState`ë¥¼ í›ˆë ¨ ë‹¨ê³„ í•¨ìˆ˜ì— ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ `flax.training.train_state.TrainState()` API ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n",
        "\n",
        "- ë¨¼ì € ì‚¬ìš©ì ì •ì˜ `flax.training.train_state.TrainState()` í´ë˜ìŠ¤ì— `key` í•„ë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "\n",
        "- ê·¸ëŸ° ë‹¤ìŒ í‚¤ ê°’(ì´ ê²½ìš° `dropout_key`)ì„ `train_state.TrainState.create()` ë©”ì„œë“œì— ì „ë‹¬í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "LlN_neeUkhTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No Dropout\n",
        "from flax.training import train_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "state = train_state.TrainState.create(\n",
        "  apply_fn=my_model.apply,\n",
        "  params=params,\n",
        "\n",
        "  tx=optax.adam(1e-3)\n",
        ")"
      ],
      "metadata": {
        "id": "odmEX7Llk9Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With Dropout\n",
        "from flax.training import train_state\n",
        "\n",
        "class TrainState(train_state.TrainState):\n",
        "  key: jax.random.KeyArray\n",
        "\n",
        "state = TrainState.create(\n",
        "  apply_fn=my_model.apply,\n",
        "  params=params,\n",
        "  key=dropout_key,\n",
        "  tx=optax.adam(1e-3)\n",
        ")"
      ],
      "metadata": {
        "id": "Rt-KkmJwk_TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ë‹¤ìŒìœ¼ë¡œ, Flax íŠ¸ë ˆì´ë‹ ë‹¨ê³„ í•¨ìˆ˜ì¸ `train_step`ì—ì„œ `dropout_key`ì—ì„œ ìƒˆ PRNG í‚¤ë¥¼ ìƒì„±í•˜ì—¬ ê° ë‹¨ê³„ì— ë“œë¡­ì•„ì›ƒì„ ì ìš©í•©ë‹ˆë‹¤. ì´ ì‘ì—…ì€ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
        "\n",
        "  - `jax.random.split()`; ë˜ëŠ”\n",
        "  - `jax.random.fold_in()`\n",
        "\n",
        "  ì¼ë°˜ì ìœ¼ë¡œ `jax.random.fold_in()`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ë¹ ë¦…ë‹ˆë‹¤. `jax.random.split()`ì„ ì‚¬ìš©í•˜ë©´ ë‚˜ì¤‘ì— ì¬ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” PRNG í‚¤ë¥¼ ë¶„í• í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ `jax.random.fold_in()`ì„ ì‚¬ìš©í•˜ë©´ 1) ê³ ìœ í•œ ë°ì´í„°ë¥¼ ì ‘ì–´ì•¼ í•˜ë©°, 2) PRNG ìŠ¤íŠ¸ë¦¼ì˜ ì‹œí€€ìŠ¤ë¥¼ ê¸¸ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "- ë§ˆì§€ë§‰ìœ¼ë¡œ, í¬ì›Œë“œ íŒ¨ìŠ¤ë¥¼ ìˆ˜í–‰í•  ë•Œ ìƒˆ PRNG í‚¤ë¥¼ `state.apply_fn()`ì— ì¶”ê°€ íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.\n"
      ],
      "metadata": {
        "id": "tWsVUdx-lNSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# No Dropout\n",
        "@jax.jit\n",
        "def train_step(state: TrainState, batch):\n",
        "\n",
        "  def loss_fn(params):\n",
        "    logits = state.apply_fn(\n",
        "      {'params': params},\n",
        "      x=batch['image'],\n",
        "\n",
        "\n",
        "      )\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "      logits=logits, labels=batch['label'])\n",
        "    return loss, logits\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, logits), grads = grad_fn(state.params)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state"
      ],
      "metadata": {
        "id": "X6yFanwwlo_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With Dropout\n",
        "@jax.jit\n",
        "def train_step(state: TrainState, batch, dropout_key):\n",
        "  dropout_train_key = jax.random.fold_in(key=dropout_key, data=state.step)\n",
        "  def loss_fn(params):\n",
        "    logits = state.apply_fn(\n",
        "      {'params': params},\n",
        "      x=batch['image'],\n",
        "      training=True,\n",
        "      rngs={'dropout': dropout_train_key}\n",
        "      )\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "      logits=logits, labels=batch['label'])\n",
        "    return loss, logits\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (loss, logits), grads = grad_fn(state.params)\n",
        "  state = state.apply_gradients(grads=grads)\n",
        "  return state"
      ],
      "metadata": {
        "id": "JQ0Itf1flqsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flax examples with dropout\n",
        "\n",
        "- WMT ê¸°ê³„ ë²ˆì—­ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµëœ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ì˜ˆì—ì„œëŠ” ë“œë¡­ì•„ì›ƒê³¼ ì£¼ì˜ ë“œë¡­ì•„ì›ƒì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "- í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì…ë ¥ ID ë°°ì¹˜ì— ë‹¨ì–´ ë“œë¡­ì•„ì›ƒì„ ì ìš©í•©ë‹ˆë‹¤. ì´ ì˜ˆì—ì„œëŠ” ì‚¬ìš©ì ì§€ì • `flax.linen.Dropout()` ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "## More Flax examples that use Module `make_rng()`\n",
        "- ì‹œí€€ìŠ¤ ê°„ ëª¨ë¸ì˜ ë””ì½”ë”ì—ì„œ ì˜ˆì¸¡ í† í°ì„ ì •ì˜í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "WKccQs3AmGYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning rate scheduling\n",
        "\n",
        "learning rateëŠ” ì‹¬ì¸µ ì‹ ê²½ë§ì„ í›ˆë ¨í•˜ëŠ” ë° ê°€ì¥ ì¤‘ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¤‘ í•˜ë‚˜ë¡œ ê°„ì£¼ë˜ì§€ë§Œ, ì´ë¥¼ ì„ íƒí•˜ê¸°ëŠ” ë§¤ìš° ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¨ìˆœíˆ ê³ ì •ëœ í•™ìŠµ ì†ë„ë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  í•™ìŠµ ì†ë„ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. ì´ ì˜ˆì—ì„œëŠ” ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤. ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‘ë™í•˜ê¸° ì „ì— `warmup_epochs` ì—í¬í¬ì— ëŒ€í•´ í•™ìŠµë¥ ì´ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•˜ëŠ” ì†Œìœ„ ì›Œë°ì—… ê¸°ê°„ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ \"SGDR: ì›œ ì¬ì‹œì‘ì„ ì‚¬ìš©í•œ í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•\" ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n",
        "\n",
        "\n",
        "ë‹¤ìŒì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
        "- í•™ìŠµ ì†ë„ ì¼ì • ì •ì˜í•˜ê¸°\n",
        "- í•´ë‹¹ ìŠ¤ì¼€ì¤„ì„ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ ëª¨ë¸ í›ˆë ¨í•˜ê¸°"
      ],
      "metadata": {
        "id": "itr3FrCSSj91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_learning_rate_fn(config, base_learning_rate, steps_per_epoch):\n",
        "  \"\"\"Creates learning rate schedule.\"\"\"\n",
        "  warmup_fn = optax.linear_schedule(\n",
        "      init_value=0., end_value=base_learning_rate,\n",
        "      transition_steps=config.warmup_epochs * steps_per_epoch)\n",
        "  cosine_epochs = max(config.num_epochs - config.warmup_epochs, 1)\n",
        "  cosine_fn = optax.cosine_decay_schedule(\n",
        "      init_value=base_learning_rate,\n",
        "      decay_steps=cosine_epochs * steps_per_epoch)\n",
        "  schedule_fn = optax.join_schedules(\n",
        "      schedules=[warmup_fn, cosine_fn],\n",
        "      boundaries=[config.warmup_epochs * steps_per_epoch])\n",
        "  return schedule_fn"
      ],
      "metadata": {
        "id": "GzcQ0rvlmZ2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ìŠ¤ì¼€ì¤„ì„ ì‚¬ìš©í•˜ë ¤ë©´ `create_learning_rate_fn` í•¨ìˆ˜ì— í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì „ë‹¬í•˜ì—¬ learning rate í•¨ìˆ˜ë¥¼ ìƒì„±í•œ ë‹¤ìŒ ì´ í•¨ìˆ˜ë¥¼ (train_stateì˜)`Optax` ì˜µí‹°ë§ˆì´ì €ì— ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ MNISTì—ì„œ ì´ ìŠ¤ì¼€ì¤„ì„ ì‚¬ìš©í•˜ë ¤ë©´ `train_step` í•¨ìˆ˜ë¥¼ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤:"
      ],
      "metadata": {
        "id": "qy9FABiPmylA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Default learning rate\n",
        "@jax.jit\n",
        "def train_step(state, batch):\n",
        "  def loss_fn(params):\n",
        "    logits = CNN().apply({'params': params}, batch['image'])\n",
        "    one_hot = jax.nn.one_hot(batch['label'], 10)\n",
        "    loss = jnp.mean(optax.softmax_cross_entropy(logits, one_hot))\n",
        "    return loss, logits\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (_, logits), grads = grad_fn(state.params)\n",
        "  new_state = state.apply_gradients(grads=grads)\n",
        "  metrics = compute_metrics(logits, batch['label'])\n",
        "\n",
        "\n",
        "  return new_state, metrics"
      ],
      "metadata": {
        "id": "Crlvh0_NnKsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate schedule\n",
        "@functools.partial(jax.jit, static_argnums=2)\n",
        "def train_step(state, batch, learning_rate_fn):\n",
        "  def loss_fn(params):\n",
        "    logits = CNN().apply({'params': params}, batch['image'])\n",
        "    one_hot = jax.nn.one_hot(batch['label'], 10)\n",
        "    loss = jnp.mean(optax.softmax_cross_entropy(logits, one_hot))\n",
        "    return loss, logits\n",
        "  grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "  (_, logits), grads = grad_fn(state.params)\n",
        "  new_state = state.apply_gradients(grads=grads)\n",
        "  metrics = compute_metrics(logits, batch['label'])\n",
        "  lr = learning_rate_fn(state.step)\n",
        "  metrics['learning_rate'] = lr\n",
        "  return new_state, metrics"
      ],
      "metadata": {
        "id": "TCoKlA-anNPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê·¸ë¦¬ê³  train_epoch í•¨ìˆ˜:"
      ],
      "metadata": {
        "id": "iVXO5Jvgn0ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Default learning rate\n",
        "def train_epoch(state, train_ds, batch_size, epoch, rng):\n",
        "  \"\"\"Trains for a single epoch.\"\"\"\n",
        "  train_ds_size = len(train_ds['image'])\n",
        "  steps_per_epoch = train_ds_size // batch_size\n",
        "  perms = jax.random.permutation(rng, len(train_ds['image']))\n",
        "  perms = perms[:steps_per_epoch * batch_size]\n",
        "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
        "  batch_metrics = []\n",
        "  for perm in perms:\n",
        "    batch = {k: v[perm, ...] for k, v in train_ds.items()}\n",
        "    state, metrics = train_step(state, batch)\n",
        "    batch_metrics.append(metrics)\n",
        "\n",
        "  # compute mean of metrics across each batch in epoch.\n",
        "  batch_metrics = jax.device_get(batch_metrics)\n",
        "  epoch_metrics = {\n",
        "      k: np.mean([metrics[k] for metrics in batch_metrics])\n",
        "      for k in batch_metrics[0]}\n",
        "\n",
        "  logging.info('train epoch: %d, loss: %.4f, accuracy: %.2f', epoch,\n",
        "               epoch_metrics['loss'], epoch_metrics['accuracy'] * 100)\n",
        "\n",
        "  return state, epoch_metrics"
      ],
      "metadata": {
        "id": "ALo4XS_ln3lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate schedule\n",
        "def train_epoch(state, train_ds, batch_size, epoch, learning_rate_fn, rng):\n",
        "  \"\"\"Trains for a single epoch.\"\"\"\n",
        "  train_ds_size = len(train_ds['image'])\n",
        "  steps_per_epoch = train_ds_size // batch_size\n",
        "  perms = jax.random.permutation(rng, len(train_ds['image']))\n",
        "  perms = perms[:steps_per_epoch * batch_size]\n",
        "  perms = perms.reshape((steps_per_epoch, batch_size))\n",
        "  batch_metrics = []\n",
        "  for perm in perms:\n",
        "    batch = {k: v[perm, ...] for k, v in train_ds.items()}\n",
        "    state, metrics = train_step(state, batch, learning_rate_fn)\n",
        "    batch_metrics.append(metrics)\n",
        "\n",
        "  # compute mean of metrics across each batch in epoch.\n",
        "  batch_metrics = jax.device_get(batch_metrics)\n",
        "  epoch_metrics = {\n",
        "      k: np.mean([metrics[k] for metrics in batch_metrics])\n",
        "      for k in batch_metrics[0]}\n",
        "\n",
        "  logging.info('train epoch: %d, loss: %.4f, accuracy: %.2f', epoch,\n",
        "               epoch_metrics['loss'], epoch_metrics['accuracy'] * 100)\n",
        "\n",
        "  return state, epoch_metrics"
      ],
      "metadata": {
        "id": "Y4eNB46En6S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê·¸ë¦¬ê³  `create_train_state` í•¨ìˆ˜"
      ],
      "metadata": {
        "id": "Wghh8ZfOoGiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Default learning rate\n",
        "def create_train_state(rng, config):\n",
        "  \"\"\"Creates initial `TrainState`.\"\"\"\n",
        "  cnn = CNN()\n",
        "  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
        "  tx = optax.sgd(config.learning_rate, config.momentum)\n",
        "  return train_state.TrainState.create(\n",
        "      apply_fn=cnn.apply, params=params, tx=tx)"
      ],
      "metadata": {
        "id": "SRp4UIl9oLi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate schedule\n",
        "def create_train_state(rng, config, learning_rate_fn):\n",
        "  \"\"\"Creates initial `TrainState`.\"\"\"\n",
        "  cnn = CNN()\n",
        "  params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
        "  tx = optax.sgd(learning_rate_fn, config.momentum)\n",
        "  return train_state.TrainState.create(\n",
        "      apply_fn=cnn.apply, params=params, tx=tx)"
      ],
      "metadata": {
        "id": "mRkdGgKwoNST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer learning\n",
        "\n",
        "\n",
        "ì´ ê°€ì´ë“œëŠ” Flaxë¥¼ ì‚¬ìš©í•œ ì „ì´ í•™ìŠµ ì›Œí¬í”Œë¡œìš°ì˜ ë‹¤ì–‘í•œ ë¶€ë¶„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì‘ì—…ì— ë”°ë¼ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ feature extractorë¡œë§Œ ì‚¬ìš©í•˜ê±°ë‚˜ ë” í° ëª¨ë¸ì˜ ì¼ë¶€ë¡œ fine-tunedí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ì´ ê°€ì´ë“œì—ì„œëŠ” ê·¸ ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤:\n",
        "\n",
        "- í—ˆê¹…í˜ì´ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  í•´ë‹¹ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì—ì„œ íŠ¹ì • í•˜ìœ„ ëª¨ë“ˆì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
        "- classifier ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "- ì‚¬ì „ í•™ìŠµëœ íŒŒë¼ë¯¸í„°ë¥¼ ìƒˆ ëª¨ë¸ êµ¬ì¡°ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.\n",
        "- Optaxë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ë‹¤ë¥¸ ë¶€ë¶„ì„ ê°œë³„ì ìœ¼ë¡œ í›ˆë ¨í•˜ê¸° ìœ„í•œ ì˜µí‹°ë§ˆì´ì €ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "- í›ˆë ¨í•  ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "\n",
        "### ì„±ëŠ¥ ì°¸ê³  ì‚¬í•­\n",
        "ì‘ì—…ì— ë”°ë¼ ì´ ê°€ì´ë“œì˜ ì¼ë¶€ ë‚´ìš©ì´ ìµœì ì´ ì•„ë‹ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ìœ„ì— ì„ í˜• ë¶„ë¥˜ê¸°ë§Œ í•™ìŠµí•˜ë ¤ëŠ” ê²½ìš° íŠ¹ì§• ì„ë² ë”©ì„ í•œ ë²ˆë§Œ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ í›¨ì”¬ ë” ë¹ ë¥¸ í•™ìŠµì´ ë  ìˆ˜ ìˆìœ¼ë©° ì„ í˜• íšŒê·€ ë˜ëŠ” ë¡œì§€ìŠ¤í‹± ë¶„ë¥˜ë¥¼ ìœ„í•œ íŠ¹ìˆ˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” ëª¨ë“  ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì´ í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "qhGpljYASmr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "agKua9vvohS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that the Transformers library doesn't use the latest Flax version.\n",
        "! pip install -q transformers[flax]\n",
        "# Install/upgrade Flax and JAX. For JAX installation with GPU/TPU support,\n",
        "# visit https://github.com/google/jax#installation.\n",
        "! pip install -U -q flax jax jaxlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA1oSV4Jo6mQ",
        "outputId": "5fd2b974-2cad-44ae-9b8e-2f1c94873912"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m936.8/936.8 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m226.2/226.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m214.2/214.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.1/210.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m204.3/204.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m197.4/197.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m189.9/189.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m185.6/185.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.1/180.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m202.0/202.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m217.3/217.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires jax>=0.3.15, but you have jax 0.3.6 which is incompatible.\n",
            "orbax 0.1.7 requires jax>=0.4.6, but you have jax 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.1/66.1 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.0/79.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a function for model loading\n",
        "\n",
        "ì‚¬ì „ í•™ìŠµëœ ë¶„ë¥˜ê¸°ë¥¼ ë¡œë“œí•˜ë ¤ë©´ í¸ì˜ë¥¼ ìœ„í•´ ë¨¼ì € Flax `Module`ê³¼ ì‚¬ì „ í•™ìŠµëœ ë³€ìˆ˜ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
        "\n",
        "ì•„ë˜ ì½”ë“œì—ì„œ `load_model` í•¨ìˆ˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ HuggingFaceì˜ `FlaxCLIPVisionModel` ëª¨ë¸ì„ ì‚¬ìš©í•˜ê³  `FlaxCLIPModule` ëª¨ë“ˆì„ ì¶”ì¶œí•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "oE0kvEzfo8U9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from IPython.display import clear_output\n",
        "from transformers import FlaxCLIPModel\n",
        "\n",
        "# Note: FlaxCLIPModel is not a Flax Module\n",
        "def load_model():\n",
        "  clip = FlaxCLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n",
        "  clear_output(wait=False) # Clear the loading messages\n",
        "  module = clip.module # Extract the Flax Module\n",
        "  variables = {'params': clip.params} # Extract the parameters\n",
        "  return module, variables"
      ],
      "metadata": {
        "id": "4J9Va_VEpKa3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`FlaxCLIPVisionModel` ìì²´ëŠ” Flax `Module`ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ì´ ì¶”ê°€ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•´ì•¼ í•œë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì„¸ìš”."
      ],
      "metadata": {
        "id": "0-SzW2g-pQF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting a submodule\n",
        "\n",
        "ìœ„ì˜ ìŠ¤ë‹ˆí«ì—ì„œ `load_model`ì„ í˜¸ì¶œí•˜ë©´ `text_model` ë° `vision_model` í•˜ìœ„ ëª¨ë“ˆë¡œ êµ¬ì„±ëœ FlaxCLIPModuleì´ ë°˜í™˜ë©ë‹ˆë‹¤.\n",
        "\n",
        "`.setup()` ë‚´ë¶€ì— ì •ì˜ëœ ë¹„ì „ ëª¨ë¸ í•˜ìœ„ ëª¨ë“ˆê³¼ ê·¸ ë³€ìˆ˜ë¥¼ ì¶”ì¶œí•˜ëŠ” ì‰¬ìš´ ë°©ë²•ì€ `clip` ëª¨ë“ˆì— `flax.linen.Module.bind`ë¥¼ ë°”ë¡œ ë’¤ì— ì‚¬ìš©í•˜ê³  ë¹„ì „ ëª¨ë¸ í•˜ìœ„ ëª¨ë“ˆì— `flax.linen.Module.unbind`ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n"
      ],
      "metadata": {
        "id": "-OW0InVspcps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import flax.linen as nn\n",
        "\n",
        "clip, clip_variables = load_model()\n",
        "vision_model, vision_model_vars = clip.bind(clip_variables).vision_model.unbind()"
      ],
      "metadata": {
        "id": "mpI3eOQOpvhx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a classifier\n",
        "\n",
        "ë¶„ë¥˜ê¸°ë¥¼ ë§Œë“¤ë ¤ë©´ `backbone`(ì‚¬ì „ í•™ìŠµëœ ë¹„ì „ ëª¨ë¸)ê³¼ `head`(ë¶„ë¥˜ê¸°) í•˜ìœ„ ëª¨ë“ˆë¡œ êµ¬ì„±ëœ ìƒˆ Flax `Module`ì„ ì •ì˜í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "DMHXdH9Np28Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  num_classes: int\n",
        "  backbone: nn.Module\n",
        "  \n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x):\n",
        "    x = self.backbone(x).pooler_output\n",
        "    x = nn.Dense(\n",
        "      self.num_classes, name='head', kernel_init=nn.zeros)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "sGX5p-2nqE5H"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë¶„ë¥˜ê¸° `model`ì„ êµ¬ì„±í•˜ê¸° ìœ„í•´ ë¹„ì „ ëª¨ë¸ ëª¨ë“ˆì´ `Classifier`ì— `backbone`ìœ¼ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ë§¤ê°œë³€ìˆ˜ ëª¨ì–‘ì„ ì¶”ë¡ í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ê°€ì§œ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ì—¬ ëª¨ë¸ì˜ `params`ë¥¼ ì„ì˜ë¡œ ì´ˆê¸°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "WmQD-adpqTTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 3\n",
        "model = Classifier(num_classes=num_classes, backbone=vision_model)\n",
        "\n",
        "x = jnp.empty((1, 224, 224, 3))\n",
        "variables = model.init(jax.random.PRNGKey(1), x)\n",
        "params = variables['params']"
      ],
      "metadata": {
        "id": "eOFAdXzbqgH9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer the parameters\n",
        "\n",
        "í˜„ì¬ `params`ëŠ” ë¬´ì‘ìœ„ì´ë¯€ë¡œ `vision_model_vars`ì—ì„œ ì‚¬ì „ í•™ìŠµëœ íŒŒë¼ë¯¸í„°ë¥¼ ì ì ˆí•œ ìœ„ì¹˜ì˜ `params` êµ¬ì¡°ë¡œ ì „ì†¡í•´ì•¼ í•©ë‹ˆë‹¤. ì´ ì‘ì—…ì€ `params` ê³ ì •ì„ í•´ì œí•˜ê³  `backbone` íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•œ ë‹¤ìŒ `params`ë¥¼ ë‹¤ì‹œ ê³ ì •í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
      ],
      "metadata": {
        "id": "PJXq_oboqmPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flax.core.frozen_dict import freeze\n",
        "\n",
        "params = params.unfreeze()\n",
        "params['backbone'] = vision_model_vars['params']\n",
        "params = freeze(params)"
      ],
      "metadata": {
        "id": "eoes1mdsq8a5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì°¸ê³ : ëª¨ë¸ì— `batch_stats`ì™€ ê°™ì€ ë‹¤ë¥¸ ë³€ìˆ˜ ì»¬ë ‰ì…˜ì´ í¬í•¨ë˜ì–´ ìˆëŠ” ê²½ìš° ì´ëŸ¬í•œ ì»¬ë ‰ì…˜ë„ ì „ì†¡í•´ì•¼ í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "ZUTK-7ASrL9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization\n",
        "\n",
        "ëª¨ë¸ì˜ ë‹¤ë¥¸ ë¶€ë¶„ì„ ê°œë³„ì ìœ¼ë¡œ í›ˆë ¨í•´ì•¼ í•˜ëŠ” ê²½ìš° ì„¸ ê°€ì§€ ì˜µì…˜ì´ ìˆìŠµë‹ˆë‹¤:\n",
        "\n",
        "1. `stop_gradient`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "2. `jax.grad`ì— ëŒ€í•œ ë§¤ê°œë³€ìˆ˜ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.\n",
        "3. ì—¬ëŸ¬ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•´ ì—¬ëŸ¬ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "ëŒ€ë¶€ë¶„ì˜ ê²½ìš° íš¨ìœ¨ì ì´ê³  ë‹¤ì–‘í•œ ë¯¸ì„¸ ì¡°ì • ì „ëµì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ ì‰½ê²Œ í™•ì¥í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ Optaxì˜ `multi_transform`ì„ í†µí•´ ì—¬ëŸ¬ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "5fE7KmDtrOnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optax.multi_transform\n",
        "\n",
        "\n",
        "`optax.multi_transform`ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë‹¤ìŒì„ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤:\n",
        "\n",
        "1. ë§¤ê°œë³€ìˆ˜ íŒŒí‹°ì…˜.\n",
        "2. íŒŒí‹°ì…˜ê³¼ í•´ë‹¹ ì˜µí‹°ë§ˆì´ì € ê°„ì˜ ë§¤í•‘.\n",
        "3. ë§¤ê°œë³€ìˆ˜ì™€ ëª¨ì–‘ì€ ê°™ì§€ë§Œ í•´ë‹¹ íŒŒí‹°ì…˜ ë ˆì´ë¸”ì´ í¬í•¨ëœ leavesë¥¼ ê°€ì§„ íŒŒì´íŠ¸ë¦¬.\n",
        "\n",
        "ìœ„ì˜ ëª¨ë¸ì— ëŒ€í•´ `optax.multi_transform`ì„ ì‚¬ìš©í•˜ì—¬ ë ˆì´ì–´ë¥¼ ê³ ì •í•˜ë ¤ë©´ ë‹¤ìŒ ì„¤ì •ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
        "\n",
        "- `trainable` íŒŒë¼ë¯¸í„°ì™€ `frozen` íŒŒë¼ë¯¸í„° íŒŒí‹°ì…˜ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "- `trainable` íŒŒë¼ë¯¸í„°ì˜ ê²½ìš° Adam(`optax.adam`) ì˜µí‹°ë§ˆì´ì €ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n",
        "- `frozen` íŒŒë¼ë¯¸í„°ì˜ ê²½ìš° `optax.set_to_zero` ì˜µí‹°ë§ˆì´ì €ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. ì´ ë”ë¯¸ ì˜µí‹°ë§ˆì´ì €ëŠ” ê·¸ë¼ë””ì–¸íŠ¸ë¥¼ 0ìœ¼ë¡œ ì„¤ì •í•˜ë¯€ë¡œ í•™ìŠµì´ ìˆ˜í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n",
        "- `flax.traverse_util.path_aware_map`ì„ ì‚¬ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„°ë¥¼ íŒŒí‹°ì…˜ì— ë§¤í•‘í•˜ê³  `backbone`ì˜ ìì€ `frozen`ìœ¼ë¡œ, ë‚˜ë¨¸ì§€ëŠ” `trainable`ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "YgvxQ1mUrfBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flax import traverse_util\n",
        "import optax\n",
        "\n",
        "partition_optimizers = {'trainable': optax.adam(5e-3), 'frozen': optax.set_to_zero()}\n",
        "param_partitions = freeze(traverse_util.path_aware_map(\n",
        "  lambda path, v: 'frozen' if 'backbone' in path else 'trainable', params))\n",
        "tx = optax.multi_transform(partition_optimizers, param_partitions)\n",
        "\n",
        "# visualize a subset of the param_partitions structure\n",
        "flat = list(traverse_util.flatten_dict(param_partitions).items())\n",
        "freeze(traverse_util.unflatten_dict(dict(flat[:2] + flat[-2:])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJis5S3IpPn2",
        "outputId": "fd80d8f5-2c7e-42b5-cff8-aa9fd61dc68a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenDict({\n",
              "    backbone: {\n",
              "        embeddings: {\n",
              "            class_embedding: 'frozen',\n",
              "            patch_embedding: {\n",
              "                kernel: 'frozen',\n",
              "            },\n",
              "        },\n",
              "    },\n",
              "    head: {\n",
              "        bias: 'trainable',\n",
              "        kernel: 'trainable',\n",
              "    },\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`differential learning rates`ë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ `optax.set_to_zero`ë¥¼ ë‹¤ë¥¸ ì˜µí‹°ë§ˆì´ì €ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆìœ¼ë©°, ì‘ì—…ì— ë”°ë¼ ë‹¤ë¥¸ ì˜µí‹°ë§ˆì´ì €ì™€ íŒŒí‹°ì…”ë‹ ì²´ê³„ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê³ ê¸‰ ì˜µí‹°ë§ˆì´ì €ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ì˜µíƒìŠ¤ì˜ ì˜µí‹°ë§ˆì´ì € ê²°í•© ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
      ],
      "metadata": {
        "id": "P8aOtOajsaqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the `TrainState`\n",
        "\n",
        "ëª¨ë“ˆ, íŒŒë¼ë¯¸í„°, ì˜µí‹°ë§ˆì´ì €ê°€ ì •ì˜ë˜ë©´ í‰ì†Œì™€ ê°™ì´ `TrainState`ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
      ],
      "metadata": {
        "id": "gPx2HDy7sj7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flax.training.train_state import TrainState\n",
        "\n",
        "state = TrainState.create(\n",
        "  apply_fn=model.apply,\n",
        "  params=params,\n",
        "  tx=tx)"
      ],
      "metadata": {
        "id": "nBhn1vn0ssDN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "optimizerê°€ ì „ëµì˜ ë™ê²° ë˜ëŠ” ë¯¸ì„¸ ì¡°ì •ì„ ì²˜ë¦¬í•˜ë¯€ë¡œ `train_step`ì„ ì¶”ê°€ë¡œ ë³€ê²½í•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ í›ˆë ¨ì„ ì •ìƒì ìœ¼ë¡œ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "xLM2cL9HswvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save and load checkpoints"
      ],
      "metadata": {
        "id": "nudAA4EsSpPV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XV-S6_P1swY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lapM_ZDvsI5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "id": "pDv9b5ZcSrx4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}